services:
 
  dpad_llc_home:
    build: ./frontend
    container_name: dpad_llc_home 
    hostname: dpad_llc_home 
    restart: always
    ports: 
      - 80:80 
      - 443:443 
    networks:
      - dpad_llc
  
  dpad_llc_api:
    build: ./backend
    container_name: dpad_llc_api 
    hostname: dpad_llc_api 
    restart: always
    ports:
      - 8082:8082
    environment:
      - WEBPAGE_API_PORT=8082
      - LOG_PATH=/app/log_files
    networks:
      - dpad_llc  
    volumes:
      - ./backend/app:/app
      - ./backend/log_files:/app/log_files/

  llama:
    image: ghcr.io/ggml-org/llama.cpp:server 
    container_name: llama-server
    restart: unless-stopped
    ports:
      - "7000:7000"
    networks:
      - dpad_llc 
    volumes:
      - ./ai_server/models:/models:ro
    command: >
      -m /models/meta-llama-3-8b-instruct.Q4_K_M.gguf
      --ctx-size 4096
      --threads 12
      --batch-size 256
      --host 0.0.0.0
      --port 7000 
    deploy:
      resources:
        limits:
          cpus: "16"
    profiles:
      - ai

networks:
  dpad_llc:
    driver: bridge
    name: dpad_llc
    external: true

